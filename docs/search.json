[
  {
    "objectID": "03_slides.html#the-plan",
    "href": "03_slides.html#the-plan",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The plan",
    "text": "The plan\n\nLocation, or central tendency\nScale, or spread\nFrom populations to samples\nSampling distributions and standard errors"
  },
  {
    "objectID": "03_slides.html#how-do-we-summarize-a-group",
    "href": "03_slides.html#how-do-we-summarize-a-group",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "How do we summarize a group?",
    "text": "How do we summarize a group?\n\n“Ladders were made to match the height of the enemy’s wall, which they measured by the layers of bricks, the side turned towards them not being thoroughly whitewashed. These were counted by many persons at once; and though some might miss the right calculation, most would hit upon it, particularly as they counted over and over again, and were no great way from the wall, but could see it easily enough for their purpose. The length required for the ladders was thus obtained, being calculated from the breadth of the brick.”\n– Thucydides, in Stigler (2016), p. 30-31"
  },
  {
    "objectID": "03_slides.html#averaging-not-a-universally-known-solution",
    "href": "03_slides.html#averaging-not-a-universally-known-solution",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Averaging not a universally known solution",
    "text": "Averaging not a universally known solution\nPer Stigler (2016):\n\nClear evidence of the use of mean in summarizing scientific measurements in late 1660s\nHipparchus (~150 BCE) or Ptolemy (150 CE) don’t seem to know\nal-Biruni (~1000 CE) reports midpoint between min and max"
  },
  {
    "objectID": "03_slides.html#an-early-example",
    "href": "03_slides.html#an-early-example",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "An early example",
    "text": "An early example\n\nKöbel’s depiction of the determination of the lawful rod (Köbel 1522) cited in Stigler (2016), p. 32"
  },
  {
    "objectID": "03_slides.html#finding-the-center",
    "href": "03_slides.html#finding-the-center",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Finding the “center”",
    "text": "Finding the “center”\nThe problem: we want a single number that best represents a whole collection of data.\nThree common answers:\n\nMode – the most frequent value\nMedian – the middle value\nMean – the arithmetic average\n\nThese can give very different answers!"
  },
  {
    "objectID": "03_slides.html#the-mode",
    "href": "03_slides.html#the-mode",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The mode",
    "text": "The mode\nDefinition: The value that appears most frequently in the dataset.\n\nIn Thucydides bricks example, it is the most commonly counted number of bricks\nCan be useful if your object is to be exactly right\nCons: Unstable. Some datasets have no mode or multiple modes\n\nImagine I measure everyone’s height to the nearest micron (millionth of a metre)."
  },
  {
    "objectID": "03_slides.html#the-median",
    "href": "03_slides.html#the-median",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The median",
    "text": "The median\nDefinition: The value that splits the dataset exactly in half. 50% of the data is above it, 50% is below it.\n\nVisual: Line everyone up by height; the person in the exact middle is the median.\n\nIf there are an even number of people we typically average the ‘two in the middle’\n\nPros: “Robust” – it ignores extreme outliers. If the richest person in the world walks into a room, the median income at most moves by one person.\nCons: Harder to use in mathematical formulas. Sometimes we want to put more weight on extreme values."
  },
  {
    "objectID": "03_slides.html#the-mean-building-notation",
    "href": "03_slides.html#the-mean-building-notation",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The mean: building notation",
    "text": "The mean: building notation\nDefinition: The arithmetic average.\n\nImagine a list of numbers: wages, ages, prices. Call this list \\(x\\), and we refer to the \\(i\\)th item of this list as \\(x_i\\)\nThe first number is \\(x_1\\), the second is \\(x_2\\), and so on.\nWe need to add them all up. Instead of writing “Sum,” we use the Greek letter Sigma: \\(\\Sigma\\)\nIf we have \\(n\\) items, the mean (written \\(\\bar{x}\\), pronounced “x-bar”) is:\n\n\\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\]\nTranslation: “Add everything up and divide by the count.”"
  },
  {
    "objectID": "03_slides.html#why-the-mean-1.-the-mean-as-a-balance-point",
    "href": "03_slides.html#why-the-mean-1.-the-mean-as-a-balance-point",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Why the mean? 1. The mean as a balance point",
    "text": "Why the mean? 1. The mean as a balance point\n\nPhysical analogy: If you placed weights on a seesaw at the location of each data point, the mean is the exact spot where the fulcrum must be placed for the seesaw to balance perfectly.\nUnlike median and mode, the mean is sensitive to extreme values (outliers)\nEvery observation “pulls” on the mean"
  },
  {
    "objectID": "03_slides.html#why-the-mean-2.-sometimes-you-want-the-outliers",
    "href": "03_slides.html#why-the-mean-2.-sometimes-you-want-the-outliers",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Why the mean? 2. Sometimes you want the outliers",
    "text": "Why the mean? 2. Sometimes you want the outliers\nImagine a game where I roll a fair die and if it is any number other than 6 I pay you £5. If I roll a six you pay me £100.\n\nModal earnings from this game: £5\nMedian earnings from this game: £5\nAverage earnings from this game: -£12.50!\nThe rare outcome matters a lot!"
  },
  {
    "objectID": "03_slides.html#when-do-these-differ",
    "href": "03_slides.html#when-do-these-differ",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "When do these differ?",
    "text": "When do these differ?\n\nMean, median, and mode can differ substantially in skewed distributions"
  },
  {
    "objectID": "03_slides.html#when-are-they-the-same",
    "href": "03_slides.html#when-are-they-the-same",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "When are they the same?",
    "text": "When are they the same?\n\nWhen the distribution is symmetric and unimodal\n\n\nTwo symmetric and unimodal distributions: Normal and t-distribution"
  },
  {
    "objectID": "03_slides.html#comparing-the-measures",
    "href": "03_slides.html#comparing-the-measures",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Comparing the measures",
    "text": "Comparing the measures\n\n\n\n\n\n\n\n\n\nFeature\nMode\nMedian\nMean\n\n\n\n\nIntuition\nMost typical\nThe middle\nBalance point\n\n\nOutliers\nIgnores them\nIgnores them (Robust)\nHeavily affected\n\n\nMath use\nDifficult\nHarder to manipulate\nEasiest in formulas"
  },
  {
    "objectID": "03_slides.html#why-is-the-mean-so-dominant",
    "href": "03_slides.html#why-is-the-mean-so-dominant",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Why is the mean so dominant?",
    "text": "Why is the mean so dominant?\n\nLots of contexts (e.g. finance) where the mean is good because it penalizes big errors.\nEase of calculation: Before computers, calculating the median required sorting all the data (very slow for big lists). Calculating the mean just required keeping a running total.\n\nAlso the function minimizing the squared errors is much easier to work with than the function minimizing the absolute errors."
  },
  {
    "objectID": "03_slides.html#the-location-and-its-errors",
    "href": "03_slides.html#the-location-and-its-errors",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The location and its “errors”",
    "text": "The location and its “errors”\nImagine you must pick one number to predict the next value in a dataset. You will be fined based on how wrong you are.\nWe call your miss the ‘error’ \\(e_i = x_i - g\\) where \\(x_i\\) is the actual value and \\(g\\) is your guess.\nThree different penalty rules lead to three different “best” answers:\n\nAll-or-nothing penalty \\(\\rightarrow\\) Mode\nLinear penalty \\(\\rightarrow\\) Median\nSquared penalty \\(\\rightarrow\\) Mean"
  },
  {
    "objectID": "03_slides.html#the-all-or-nothing-game",
    "href": "03_slides.html#the-all-or-nothing-game",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The all-or-nothing game",
    "text": "The all-or-nothing game\nThe rule: If you are exactly right, you pay nothing. If you are wrong by any amount, you pay £100.\nBest strategy: Pick the Mode. (small caveat: the mode needs to exist)\n\nIt gives you the highest probability of hitting the number exactly.\n\nLoss function: \\(L(e) = 100 \\times \\mathbb{I}(e \\neq 0)\\)"
  },
  {
    "objectID": "03_slides.html#the-linear-penalty-game",
    "href": "03_slides.html#the-linear-penalty-game",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The linear penalty game",
    "text": "The linear penalty game\nThe rule: You pay £1 for every unit you are away from the truth. Being off by 10 costs £10; being off by 100 costs £100.\nBest strategy: Pick the Median.\n\nIt minimizes the sum of absolute deviations\nBalances the distance on the left and right sides\nLoss function: \\(L(e) = 100 \\times |e|\\)"
  },
  {
    "objectID": "03_slides.html#the-squared-penalty-game",
    "href": "03_slides.html#the-squared-penalty-game",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The squared penalty game",
    "text": "The squared penalty game\n\nThe rule: You pay based on the square of your error.\n\nOff by 1? Pay £1.\nOff by 10? Pay £100.\nOff by 50? Pay £2,500.\n\nThe logic: Small mistakes are fine, but big mistakes are disastrous.\nBest strategy: Pick the Mean.\n\nIt minimizes the sum of squared errors (Least Squares)\n\nLoss function: \\(L(e) = 100 \\times e^2\\)"
  },
  {
    "objectID": "03_slides.html#from-location-to-spread",
    "href": "03_slides.html#from-location-to-spread",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "From location to spread",
    "text": "From location to spread\n\nThe problem: Knowing the average isn’t enough."
  },
  {
    "objectID": "03_slides.html#the-average-squared-error",
    "href": "03_slides.html#the-average-squared-error",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The average squared error",
    "text": "The average squared error\n\nRecall: The mean is the number that minimizes the sum of squared errors.\nThe question: If the mean is our “best guess,” how good is that guess on average?\n\nCalculate the squared distance of every point from the mean\nAdd them up and divide by the count\nThis gives us a measure of the average dispersion of the data"
  },
  {
    "objectID": "03_slides.html#variance-definition",
    "href": "03_slides.html#variance-definition",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Variance: definition",
    "text": "Variance: definition\n\nDefinition: The Variance (\\(s^2\\)) is the average squared distance from the center.\n\\[s^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\]\nTranslation: “How far, on average, are the data points from the mean (squared)?”\nRemember we defined: \\(e_i = x_i - \\bar{x}\\).\nSo we can rewrite the variance as:\n\\[s^2 = \\frac{1}{n}\\sum_{i=1}^{n} e_i^2\\]"
  },
  {
    "objectID": "03_slides.html#the-units-problem",
    "href": "03_slides.html#the-units-problem",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The units problem",
    "text": "The units problem\n\n\nIf we measure the age at death of Roman Emperors in years, the variance is measured in years squared\nWhat is a “square year”? It doesn’t make intuitive sense historically.\nThis is awkward for interpretating variance on the scale of the data"
  },
  {
    "objectID": "03_slides.html#standard-deviation-returning-to-regular-units",
    "href": "03_slides.html#standard-deviation-returning-to-regular-units",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Standard deviation: returning to regular units",
    "text": "Standard deviation: returning to regular units\n\nThe fix: Take the square root of the variance to put the spread in units that are the same as the data.\nDefinition: The Standard Deviation (\\(s\\))\n\\[s = \\sqrt{s^2} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} e_i^2}\\]\nwhich equals:\n\\[s = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} x_i^2 - \\bar{x}^2}\\]\nInterpretation: Roughly the “average” distance of a data point from the mean."
  },
  {
    "objectID": "03_slides.html#what-does-the-standard-deviation-tell-us",
    "href": "03_slides.html#what-does-the-standard-deviation-tell-us",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "What does the standard deviation tell us?",
    "text": "What does the standard deviation tell us?\n\n\nLow SD: Data is close to the mean\n\nA more homogeneous population\n\nHigh SD: Data is far from the mean\n\nA more heterogeneous population\n\nTwo datasets with the same mean but different SDs describe very different worlds"
  },
  {
    "objectID": "03_slides.html#visualizing-dispersion",
    "href": "03_slides.html#visualizing-dispersion",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Visualizing dispersion",
    "text": "Visualizing dispersion\n\n\n\n\n\n\nTwo distributions with the same mean but different standard deviations"
  },
  {
    "objectID": "03_slides.html#other-measures-of-dispersion",
    "href": "03_slides.html#other-measures-of-dispersion",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Other measures of dispersion",
    "text": "Other measures of dispersion\n\nSometimes (not often) you will see other measures of dispersion e.g.\n\nThe range: the difference between the maximum and minimum value\nThe interquartile range: the difference between the 75th and 25th percentile\nThe mean absolute deviation: the average absolute distance from the mean\nThe median absolute deviation: the median of the absolute distances from the median"
  },
  {
    "objectID": "03_slides.html#means-and-sample-averages",
    "href": "03_slides.html#means-and-sample-averages",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Means and sample averages",
    "text": "Means and sample averages\n\n\nImagine a well defined population like “the heights (in cm) of all students currently registered in the history department at KCL”\n\nImagine this is 1000 students\n\nThis population has a mean which we will call \\(\\mu\\) (it is convention to use the greek letter mu for the population mean)\nThis population has a standard deviation which we will call \\(\\sigma\\) (it is convention to use the greek letter sigma for the population standard deviation)\nI take the heights of the ~20 history students in this class and calculate the average height \\(\\bar{x}\\)\n\nWhat is the relationship between \\(\\bar{x}\\) and \\(\\mu\\)?"
  },
  {
    "objectID": "03_slides.html#means-and-sample-averages-visualized",
    "href": "03_slides.html#means-and-sample-averages-visualized",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Means and sample averages: visualized",
    "text": "Means and sample averages: visualized\n\n\n\n\n\n\nSample vs population"
  },
  {
    "objectID": "03_slides.html#sample-vs-population",
    "href": "03_slides.html#sample-vs-population",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Sample vs population",
    "text": "Sample vs population\n\n\nIn practice we rarely observe the full population\nWe observe a sample and want to learn about the population\nSample statistics are estimates of population parameters\n\n\n\n\nPopulation parameter\nSample statistic\n\n\n\n\n\\(\\mu\\) (mean)\n\\(\\bar{x}\\)\n\n\n\\(\\sigma^2\\) (variance)\n\\(s^2\\)\n\n\n\\(\\sigma\\) (std dev)\n\\(s\\)"
  },
  {
    "objectID": "03_slides.html#the-sampling-distribution",
    "href": "03_slides.html#the-sampling-distribution",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The sampling distribution",
    "text": "The sampling distribution\n\n\nA sample mean is itself subject to sampling error\nIf we drew many samples from the same population, each would give a different \\(\\bar{x}\\)\nThe distribution of these sample means is called the sampling distribution\nThis distribution has its own spread, which we can quantify"
  },
  {
    "objectID": "03_slides.html#the-standard-error-of-the-mean",
    "href": "03_slides.html#the-standard-error-of-the-mean",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The standard error of the mean",
    "text": "The standard error of the mean\n\nThe standard error of the mean (SEM) describes how much sample means vary:\n\\[SE(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}\\]\n\nKey insight: the standard error decreases as sample size increases\nLarger samples give more precise estimates"
  },
  {
    "objectID": "03_slides.html#standard-deviation-vs-standard-error",
    "href": "03_slides.html#standard-deviation-vs-standard-error",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Standard deviation vs standard error",
    "text": "Standard deviation vs standard error\n\n\n\n\n\n\n\n\n\n\nStandard Deviation (SD)\nStandard Error (SE)\n\n\n\n\nMeasures\nSpread of individual observations\nSpread of sample means\n\n\nQuestion\nHow variable is the data?\nHow uncertain is our estimate of the mean?\n\n\nFormula\n\\(s = \\sqrt{\\frac{1}{n}\\sum(x_i - \\bar{x})^2}\\)\n\\(SE = \\frac{s}{\\sqrt{n}}\\)\n\n\nAs n increases\nStays roughly the same\nGets smaller\n\n\nDescribes\nThe population/sample itself\nOur knowledge about the population"
  },
  {
    "objectID": "03_slides.html#intuition-for-the-standard-error",
    "href": "03_slides.html#intuition-for-the-standard-error",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Intuition for the standard error",
    "text": "Intuition for the standard error\n\n\n\n\n\n\nSampling distribution of the mean narrows as sample size increases"
  },
  {
    "objectID": "03_slides.html#why-the-standard-error-matters",
    "href": "03_slides.html#why-the-standard-error-matters",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Why the standard error matters",
    "text": "Why the standard error matters\n\n\nThe standard error lets us quantify uncertainty about our estimate\nLarger samples give more precise estimates (smaller SE)\nThis is the foundation for:\n\nConfidence intervals\nHypothesis testing\nAssessing statistical significance"
  },
  {
    "objectID": "03_slides.html#the-standard-error-in-historical-practice",
    "href": "03_slides.html#the-standard-error-in-historical-practice",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The standard error in historical practice",
    "text": "The standard error in historical practice\n\nConsider a historian estimating average wages in 18th century London:\n\nSample of 500 wage records: \\(\\bar{x}\\) = 12 shillings, \\(s\\) = 4 shillings\n\\(SE = \\frac{4}{\\sqrt{500}} \\approx 0.18\\) shillings\nWe can be confident the true mean is close to 12 shillings\nBut what if we only had 25 records?\n\n\\(SE = \\frac{4}{\\sqrt{25}} = 0.8\\) shillings – much more uncertainty!"
  },
  {
    "objectID": "03_slides.html#statistical-vs-substantive-significance",
    "href": "03_slides.html#statistical-vs-substantive-significance",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Statistical vs substantive significance",
    "text": "Statistical vs substantive significance\n\n\nThe standard error is what is used to determine if something is ‘statistically significantly different’ from another thing (usually zero)\nThat is because confidence intervals are constructed using the standard error (we discussed CIs last week)\nThe bigger your sample the smaller the CI\nMcCloskey and Ziliak (1996) insists we should not confuse statistical significance with substantive significance\n\nStatistical significance is about how precisely we measure something, not how big something is."
  },
  {
    "objectID": "03_slides.html#discussion-questions",
    "href": "03_slides.html#discussion-questions",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Discussion questions",
    "text": "Discussion questions\n\n\nWhen might the median be preferable to the mean in historical research?\nFeinstein and Thomas (2002) and Hudson and Ishizu (2016) present these concepts for historians. Why might historians need different pedagogical approaches than economists?\nHow should we think about the standard error when our data is not a random sample? (Most historical data isn’t!)"
  },
  {
    "objectID": "03_slides.html#exercise-british-wages",
    "href": "03_slides.html#exercise-british-wages",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Exercise: British wages",
    "text": "Exercise: British wages\n\nI will give you data on wages in different historical periods.\n\nCalculate the mean, median, and standard deviation\nCalculate the standard error of the mean\nWhat does the standard error tell you about how confident you should be in your estimate?\nWhat doesn’t the standard error tell you about the quality of your data?"
  },
  {
    "objectID": "03_slides.html#key-takeaways",
    "href": "03_slides.html#key-takeaways",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Key takeaways",
    "text": "Key takeaways\n\n\nLocation (mean, median, mode) tells us where the center is – each minimizes a different type of error\nScale (variance, standard deviation) tells us how spread out the data is\nThe standard error tells us how uncertain we are about our estimate of the mean (it is the standard deviation of our parameter of interest)"
  },
  {
    "objectID": "03_slides.html#bibliography",
    "href": "03_slides.html#bibliography",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Bibliography",
    "text": "Bibliography\n\n\n\n\n\n\n\n\nMcCloskey, Deirdre N., and Stephen T. Ziliak. 1996. “The Standard Error of Regressions.” Journal of Economic Literature 34 (1): 97–114. http://www.jstor.org/stable/2729411.\n\n\nStigler, Stephen M. 2016. The seven pillars of statistical wisdom. Cambridge, Massachusetts London, England: Harvard University Press."
  },
  {
    "objectID": "03_slides.html#means-and-sample-averages-1",
    "href": "03_slides.html#means-and-sample-averages-1",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Means and sample averages",
    "text": "Means and sample averages\n\n\n\n\n\n\nSample vs population"
  },
  {
    "objectID": "03_slides.html#the-standard-error-and-sample-size",
    "href": "03_slides.html#the-standard-error-and-sample-size",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The standard error and sample size",
    "text": "The standard error and sample size\n\n\n\n\n\n\nSample mean and ±2 SD for different sample sizes"
  },
  {
    "objectID": "04_slides.html#the-plan",
    "href": "04_slides.html#the-plan",
    "title": "Selection bias",
    "section": "The plan",
    "text": "The plan\n\nThe essay\nWhat is sample selection bias?\nSelection on observables\nSelection on unobservables\nHow selection bias distorts relationships between variables"
  },
  {
    "objectID": "04_slides.html#from-statistical-significance-to-bias",
    "href": "04_slides.html#from-statistical-significance-to-bias",
    "title": "Selection bias",
    "section": "From statistical significance to bias",
    "text": "From statistical significance to bias\n\nLast week we covered means, standard errors, and statistical significance\nThose tools assume our sample is a random draw from the population\nBut what happens when it isn’t?\nIf our sample is not random, our estimates may be biased – systematically wrong in a particular direction\nNo amount of data fixes a biased sample: a bigger biased sample is still biased"
  },
  {
    "objectID": "04_slides.html#the-bigger-threat",
    "href": "04_slides.html#the-bigger-threat",
    "title": "Selection bias",
    "section": "The bigger threat",
    "text": "The bigger threat\n\nIn observational studies the bigger danger is not sampling error but that your sample is systematically unrepresentative of the population\nStatistical significance tells us about precision (how much noise is in our estimate)\nBut bias is about accuracy (whether we are pointing at the right answer)\nYou can have a very precisely measured estimates of the wrong thing"
  },
  {
    "objectID": "04_slides.html#what-is-sample-selection-bias",
    "href": "04_slides.html#what-is-sample-selection-bias",
    "title": "Selection bias",
    "section": "What is sample selection bias?",
    "text": "What is sample selection bias?\nDefinition: Sample selection bias occurs when your sample is not a random draw from the population you want to study. You are more likely to capture some kinds of people rather than others.\n\nThe process that generates your data is not independent of the thing you are trying to measure\nThis means your sample statistics (means, correlations, etc.) may not reflect the true population values"
  },
  {
    "objectID": "04_slides.html#where-does-historical-data-come-from",
    "href": "04_slides.html#where-does-historical-data-come-from",
    "title": "Selection bias",
    "section": "Where does historical data come from?",
    "text": "Where does historical data come from?\n\n\nTax records – only people above a wealth threshold\nMilitary records – only people who enlisted (and met requirements)\nParish registers – only people in that parish, and that religion\nCourt records – only people involved in disputes\nCensus records – depends on who the enumerator found at home\nNewspaper reports – depends on what editors found newsworthy\n\nNone of these are random samples of the population. Each source has its own selection process.\nAlways ask: who is in the sample and why?"
  },
  {
    "objectID": "04_slides.html#a-simple-example-height-and-the-military",
    "href": "04_slides.html#a-simple-example-height-and-the-military",
    "title": "Selection bias",
    "section": "A simple example: height and the military",
    "text": "A simple example: height and the military\n\n\nSuppose we want to estimate average height in the population using military records\nBut the military has a minimum height requirement – say, 165 cm\nEveryone below the cutoff is excluded from our data\nOur sample mean will overestimate the true population mean\nThis is truncation bias: we only observe part of the distribution"
  },
  {
    "objectID": "04_slides.html#truncation-bias-illustrated",
    "href": "04_slides.html#truncation-bias-illustrated",
    "title": "Selection bias",
    "section": "Truncation bias illustrated",
    "text": "Truncation bias illustrated\n\n\n\n\n\n\nMinimum height requirement biases the sample mean upward"
  },
  {
    "objectID": "04_slides.html#aside-truncation-and-worst-case-bounds",
    "href": "04_slides.html#aside-truncation-and-worst-case-bounds",
    "title": "Selection bias",
    "section": "Aside: truncation and worst-case bounds",
    "text": "Aside: truncation and worst-case bounds\n\n\nWhen your sample is truncated (e.g., only observe heights above a threshold), you can place bounds on what the population mean might be\nThe worst case: all the people you don’t observe could be very short (lower bound) or very tall (upper bound)\nTruncation alone is tractable – we can model it if we know the cutoff\nBut the real problem in historical data is usually more subtle: selection depends on things we cannot fully observe"
  },
  {
    "objectID": "04_slides.html#selection-on-observables",
    "href": "04_slides.html#selection-on-observables",
    "title": "Selection bias",
    "section": "Selection on observables",
    "text": "Selection on observables\nDefinition: Selection bias that depends on variables you can see in your data.\n\nIf you know what is causing the selection, you can potentially correct for it\nThe key requirement: the variable driving selection must be measured in your dataset"
  },
  {
    "objectID": "04_slides.html#a-polling-example",
    "href": "04_slides.html#a-polling-example",
    "title": "Selection bias",
    "section": "A polling example",
    "text": "A polling example\n\nSuppose you’re polling voting intentions\nYour sample over-represents university-educated voters (60% of sample vs. 30% of population)\nUniversity-educated voters favour Party A at 70%; non-university voters favour Party A at 40%\nNaive estimate (raw sample): \\(0.6 \\times 70 + 0.4 \\times 40 = 58\\%\\) for Party A\nReweighted estimate (using population shares): \\(0.3 \\times 70 + 0.7 \\times 40 = 49\\%\\) for Party A\nBecause we observed education level, we could fix the bias"
  },
  {
    "objectID": "04_slides.html#reweighting-in-action",
    "href": "04_slides.html#reweighting-in-action",
    "title": "Selection bias",
    "section": "Reweighting in action",
    "text": "Reweighting in action\n\n\n\n\n\n\nReweighting corrects for over-representation of university-educated voters"
  },
  {
    "objectID": "04_slides.html#selection-on-unobservables",
    "href": "04_slides.html#selection-on-unobservables",
    "title": "Selection bias",
    "section": "Selection on unobservables",
    "text": "Selection on unobservables\nDefinition: Selection bias that depends on variables you cannot see in your data.\n\nYou cannot reweight or control for something you haven’t measured\nThis is the hard problem: the bias is invisible in your dataset\nYou need assumptions or external information to address it"
  },
  {
    "objectID": "04_slides.html#the-anthropometrics-problem",
    "href": "04_slides.html#the-anthropometrics-problem",
    "title": "Selection bias",
    "section": "The anthropometrics problem",
    "text": "The anthropometrics problem\n\n\nHistorians use military height records to infer population health and nutrition\nMilitary recruiters selected soldiers partly based on characteristics we can observe (age, occupation)\nBut also on characteristics we cannot observe: health, motivation, economic desperation\nIf unhealthy or desperate people are more likely to enlist (because they lack other options), the height sample may be biased downward\nAnd you can’t fix this by reweighting – because you don’t observe the variable driving selection"
  },
  {
    "objectID": "04_slides.html#the-industrialization-puzzle",
    "href": "04_slides.html#the-industrialization-puzzle",
    "title": "Selection bias",
    "section": "The industrialization puzzle",
    "text": "The industrialization puzzle\n\n\nBodenhorn, Guinnane, and Mroz (2017) argue that US military height samples are selected on unobservables\nThe observed decline in heights during industrialization may be an artefact of changing selection patterns, not a real decline in nutrition\nAs the economy industrialized, the type of person who enlisted changed\nIf selection into the military became less favourable over time (e.g., more desperate, less healthy recruits), measured heights would fall even if population health was stable\nKomlos and A’Hearn (2019) dispute this interpretation; Bodenhorn, Guinnane, and Mroz (2019) respond"
  },
  {
    "objectID": "04_slides.html#the-heckman-correction-the-problem",
    "href": "04_slides.html#the-heckman-correction-the-problem",
    "title": "Selection bias",
    "section": "The Heckman correction: the problem",
    "text": "The Heckman correction: the problem\n\n\nWe want to estimate average height in the population\nBut we only observe heights of people who were selected into the military\nThe selection process is correlated with height itself (or with things correlated with height)\nSimply averaging observed heights gives a biased answer\nWe need some way to account for the selection process"
  },
  {
    "objectID": "04_slides.html#the-heckman-correction-the-intuition",
    "href": "04_slides.html#the-heckman-correction-the-intuition",
    "title": "Selection bias",
    "section": "The Heckman correction: the intuition",
    "text": "The Heckman correction: the intuition\n\n\nHeckman’s insight: Model the selection process separately from the outcome\nUse information about who gets selected to adjust your estimates of the outcome\nAnalogy: Imagine you only see wages of people who are employed. To estimate what wages would look like for everyone, you first need to model who chooses to work and who doesn’t. The factors that predict employment (but not wages directly) help you correct for the bias.\nLimitation: Requires assumptions about the functional form of the selection process – if those assumptions are wrong, the correction can be wrong too\nBodenhorn, Guinnane, and Mroz (2017) use a Heckman-style correction; Komlos and A’Hearn (2019) dispute whether the assumptions are justified"
  },
  {
    "objectID": "04_slides.html#selection-bias-and-relationships-between-variables",
    "href": "04_slides.html#selection-bias-and-relationships-between-variables",
    "title": "Selection bias",
    "section": "Selection bias and relationships between variables",
    "text": "Selection bias and relationships between variables\n\nSo far we’ve focused on how selection bias shifts averages\nBut selection bias can also distort relationships between variables\nThis is sometimes called Berkson’s paradox or collider bias\nThe key insight: conditioning on a variable that is caused by two other variables can create a spurious correlation between them"
  },
  {
    "objectID": "04_slides.html#berksons-paradox-the-nba-example",
    "href": "04_slides.html#berksons-paradox-the-nba-example",
    "title": "Selection bias",
    "section": "Berkson’s paradox: the NBA example",
    "text": "Berkson’s paradox: the NBA example\n\nAmong NBA players, taller players tend to have worse free-throw percentages\nBut in the general population, height has no relationship to free-throw accuracy\nThe NBA selects players who are either very tall or very accurate shooters (or both)\nAmong the selected group, if you’re not tall, you must be an amazing shooter (otherwise you wouldn’t be in the NBA)\nThis creates a spurious negative correlation in the selected sample that doesn’t exist in the population"
  },
  {
    "objectID": "04_slides.html#berksons-paradox-illustrated",
    "href": "04_slides.html#berksons-paradox-illustrated",
    "title": "Selection bias",
    "section": "Berkson’s paradox illustrated",
    "text": "Berkson’s paradox illustrated\n\nSelection creates a spurious negative correlation (Berkson’s paradox)"
  },
  {
    "objectID": "04_slides.html#connection-to-the-anthropometrics-debate",
    "href": "04_slides.html#connection-to-the-anthropometrics-debate",
    "title": "Selection bias",
    "section": "Connection to the anthropometrics debate",
    "text": "Connection to the anthropometrics debate\n\n\nIn the height debates, if selection patterns changed over time (e.g., the military became less selective during wartime), the relationship between height and time is distorted\nIf during peacetime only the tallest and healthiest enlist, but during wartime the military takes almost everyone, the average height of recruits falls – but not because the population got shorter\nThis is the same logic as Berkson’s paradox: conditioning on selection distorts the relationship between the variables of interest\nThis is what makes selection on unobservables so dangerous for historical inference"
  },
  {
    "objectID": "04_slides.html#key-takeaways",
    "href": "04_slides.html#key-takeaways",
    "title": "Selection bias",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nSample selection bias is often a bigger threat than sampling error in historical research\nSelection on observables can be corrected if you measure the relevant variables\nSelection on unobservables requires strong assumptions to correct (e.g., Heckman’s joint normality)\nSelection can distort relationships between variables (Berkson’s paradox), not just averages\nAlways ask: who is in the sample and why?"
  },
  {
    "objectID": "04_slides.html#bibliography",
    "href": "04_slides.html#bibliography",
    "title": "Selection bias",
    "section": "Bibliography",
    "text": "Bibliography\n\n\n\n\n\n\n\n\nManski, Charles F. 2007. Identification for Predication and Decision. Harvard University Press."
  },
  {
    "objectID": "04_slides.html#a-simple-example-height-and-the-royal-marines",
    "href": "04_slides.html#a-simple-example-height-and-the-royal-marines",
    "title": "Selection bias",
    "section": "A simple example: height and the Royal Marines",
    "text": "A simple example: height and the Royal Marines\n\nSuppose we want to estimate average height in the population using military records\nThe Royal Marines require a minimum height of 145 cm\nEveryone below the cutoff is excluded from our data\nOur sample mean will overestimate the true population mean\nThis is truncation bias: we only observe part of the distribution"
  },
  {
    "objectID": "04_slides.html#worst-case-bounds-and-missing-data",
    "href": "04_slides.html#worst-case-bounds-and-missing-data",
    "title": "Selection bias",
    "section": "Worst-case bounds and missing data",
    "text": "Worst-case bounds and missing data\nManski (2007) : A survey contacts 137 unhoused people and follows up later to ask whether they found housing.\n\nAt follow-up, only 78 respond. Of these, 24 had exited homelessness.\n59 non-respondents: did they find housing? We don’t know.\nLower bound: Assume all 59 non-respondents did not exit \\(\\rightarrow\\) \\(24/137 = 17.5\\%\\)\nUpper bound: Assume all 59 non-respondents did exit \\(\\rightarrow\\) \\((24 + 59)/137 = 60.6\\%\\)\nNaive estimate (respondents only): \\(24/78 = 30.8\\%\\)\nThe true answer lies somewhere in \\([17.5\\%, 60.6\\%]\\) – a wide range, but an honest one"
  },
  {
    "objectID": "04_slides.html#correcting-for-selection-on-unobservables",
    "href": "04_slides.html#correcting-for-selection-on-unobservables",
    "title": "Selection bias",
    "section": "Correcting for selection on unobservables",
    "text": "Correcting for selection on unobservables\n\nThe Heckman correction attempts to fix selection on unobservables\nKey assumption: the errors in the outcome equation and the selection equation are jointly normally distributed\nHigh level overview: you assume a specific formula describes the relationship between the unobservables driving selection and the outcome."
  },
  {
    "objectID": "04_slides.html#the-heckman-correction-in-action",
    "href": "04_slides.html#the-heckman-correction-in-action",
    "title": "Selection bias",
    "section": "The Heckman correction in action",
    "text": "The Heckman correction in action\n\n\n\n\n\n\nUnder joint normality, the Heckman correction recovers the true mean"
  },
  {
    "objectID": "04_slides.html#worse-case-bounds",
    "href": "04_slides.html#worse-case-bounds",
    "title": "Selection bias",
    "section": "Worse-case bounds",
    "text": "Worse-case bounds"
  },
  {
    "objectID": "04_slides.html#your-essay",
    "href": "04_slides.html#your-essay",
    "title": "Selection bias",
    "section": "Your essay",
    "text": "Your essay\n3,000 words.\nDue on 28 April 2026 before 14:00"
  },
  {
    "objectID": "04_slides.html#essay-question",
    "href": "04_slides.html#essay-question",
    "title": "Selection bias",
    "section": "Essay question",
    "text": "Essay question\nDiscuss the role of theory and measurement in a historiographical dispute in economic history.\nExamples you can use:\n\nThe debate over heights, well-being, and the ‘industrialization’ puzzle.\nThe debate over the ‘high wage economy’ thesis and why Britain industrialized first.\nThe debate over the gold standard, empire, and sovereign bond yields in the 19th century.\nThe debate over the measurement of income inequality in the 20th century united states."
  },
  {
    "objectID": "04_slides.html#essay-question-1",
    "href": "04_slides.html#essay-question-1",
    "title": "Selection bias",
    "section": "Essay question",
    "text": "Essay question\nYou can write your own but:\n\nIt needs to be a scholarly dispute\nIt needs to be quantitative\nI need to approve it"
  },
  {
    "objectID": "04_slides.html#essay-practice",
    "href": "04_slides.html#essay-practice",
    "title": "Selection bias",
    "section": "Essay practice",
    "text": "Essay practice\nI will have you write an outline first, and will grade it for you.\nWe can pick the due date for that together."
  },
  {
    "objectID": "05_slides.html#the-plan",
    "href": "05_slides.html#the-plan",
    "title": "Linear Regression",
    "section": "The plan",
    "text": "The plan\n\nA motivating example\nComparing averages by gender\nAveraging by date of birth\nConditional averages and linear regression\nThe regression equation and OLS\nErrors vs residuals\nStatistical uncertainty\nReading regression tables\nMultiple regression"
  },
  {
    "objectID": "05_slides.html#a-motivating-example",
    "href": "05_slides.html#a-motivating-example",
    "title": "Linear Regression",
    "section": "A motivating example",
    "text": "A motivating example\n\nImagine we are conducting an anthropometric study using a dataset of 1,517 heights recorded in 1850\n1,215 have recorded gender as male and 302 as female\nFor each record we also measure the person’s date of birth\nWe want to understand the relationship between height, gender, and date of birth\nWe can think of regression as a way of comparing averages (Gelman, Hill, and Vehtari 2020)"
  },
  {
    "objectID": "05_slides.html#a-snippet-of-the-data",
    "href": "05_slides.html#a-snippet-of-the-data",
    "title": "Linear Regression",
    "section": "A snippet of the data",
    "text": "A snippet of the data\n\n\n\n\nTable 1: A snippet of the height data\n\n\n\n\n\n\n\nIndividual\nHeight (cm)\nGender\nDOB\n\n\n\n\n1\n177.44\nM\n1826-10-28\n\n\n2\n167.63\nM\n1829-05-14\n\n\n3\n176.31\nM\n1829-02-20\n\n\n1517\n162.86\nF\n1824-01-18"
  },
  {
    "objectID": "05_slides.html#comparing-averages-by-gender",
    "href": "05_slides.html#comparing-averages-by-gender",
    "title": "Linear Regression",
    "section": "Comparing averages by gender",
    "text": "Comparing averages by gender\nWe can calculate averages separately by gender:\n\\[\n\\text{Avg}_M = \\sum_{i=1}^{1215} \\frac{h_i}{1215} \\qquad \\text{Avg}_F = \\sum_{i=1}^{302} \\frac{h_i}{302}\n\\]"
  },
  {
    "objectID": "05_slides.html#precision-differs-by-group-size",
    "href": "05_slides.html#precision-differs-by-group-size",
    "title": "Linear Regression",
    "section": "Precision differs by group size",
    "text": "Precision differs by group size\n\nAssume the standard deviation of men’s and women’s heights is the same value \\(\\sigma\\)\nStandard error of male heights: \\(\\sigma_M = \\frac{\\sigma}{\\sqrt{1215}} \\approx \\frac{\\sigma}{35}\\)\nStandard error of female heights: \\(\\sigma_F = \\frac{\\sigma}{\\sqrt{302}} \\approx \\frac{\\sigma}{17}\\)\nOur measure of male heights is about 2 times as accurate as our measure of female heights\nThis occurs simply because we observe fewer women in the data"
  },
  {
    "objectID": "05_slides.html#heights-by-gender",
    "href": "05_slides.html#heights-by-gender",
    "title": "Linear Regression",
    "section": "Heights by gender",
    "text": "Heights by gender\n\n\nFigure 1: Heights by gender. Red points and error bars show the mean \\(\\pm\\) 2 standard errors."
  },
  {
    "objectID": "05_slides.html#averaging-by-date-of-birth",
    "href": "05_slides.html#averaging-by-date-of-birth",
    "title": "Linear Regression",
    "section": "Averaging by date of birth",
    "text": "Averaging by date of birth\nWhat if we want to calculate the average by date of birth?\n\n\nFigure 2: Individual heights plotted against date of birth in 1822."
  },
  {
    "objectID": "05_slides.html#the-problem-with-fine-grained-averages",
    "href": "05_slides.html#the-problem-with-fine-grained-averages",
    "title": "Linear Regression",
    "section": "The problem with fine-grained averages",
    "text": "The problem with fine-grained averages\n\nIf we reduce DOB to the year of birth we can calculate an average, but it is less precise\nAt the year-and-month level it becomes almost impossible\nAt the actual day of birth most days have 1 or no observations\nWe need an approach that uses all of the observed data and generalizes to any date"
  },
  {
    "objectID": "05_slides.html#thinking-about-prediction",
    "href": "05_slides.html#thinking-about-prediction",
    "title": "Linear Regression",
    "section": "Thinking about prediction",
    "text": "Thinking about prediction\n\nLet’s shift perspective: consider a date we have no observations for\nWhat is a good strategy for guessing the average height at this date?\n\n\n\nFigure 3: Average height by week. Point size indicates the number of observations."
  },
  {
    "objectID": "05_slides.html#interpolation",
    "href": "05_slides.html#interpolation",
    "title": "Linear Regression",
    "section": "Interpolation",
    "text": "Interpolation\n\nSay we observe average heights for February 1822 and April 1822\nA reasonable guess for March 1822:\n\n\\[\n\\hat{h}_{1822\\text{-Mar}} = h_{1822\\text{-Feb}} + \\frac{h_{1822\\text{-Apr}} - h_{1822\\text{-Feb}}}{t_{1822\\text{-Apr}} - t_{1822\\text{-Feb}}}\n\\]\n\nThis is the rise over run: change in height divided by change in time"
  },
  {
    "objectID": "05_slides.html#limitations-of-interpolation",
    "href": "05_slides.html#limitations-of-interpolation",
    "title": "Linear Regression",
    "section": "Limitations of interpolation",
    "text": "Limitations of interpolation\n\nThe individual monthly observations are based on few observations — they may not be very accurate\nWe observe many data points — how do we include all of them?\nWhat if we are missing two observations in a row?\nWe are working with time averages but really we see the day people are born\nWe need an approach that:\n\nUses all of the observed data\nGeneralizes to any date with missing values"
  },
  {
    "objectID": "05_slides.html#conditional-averages",
    "href": "05_slides.html#conditional-averages",
    "title": "Linear Regression",
    "section": "Conditional averages",
    "text": "Conditional averages\n\nThe best case: so many observations per date that we could calculate the average for each day\nThis would be the conditional average: the average conditional on the day a person was born\nWe don’t have enough data for this, but at least we know our goal is a conditional average\nSolution: calculate an average that depends on the date and a very small number of unknown parameters"
  },
  {
    "objectID": "05_slides.html#linear-regression",
    "href": "05_slides.html#linear-regression",
    "title": "Linear Regression",
    "section": "Linear regression",
    "text": "Linear regression\nLinear regression computes a linear approximation to the conditional average.\nThe word linear means:\n\nThe relationship is the same no matter what time period we look at: moving from 1822-02-10 to 1822-02-20 has the same effect as moving from 1823-02-10 to 1823-02-20\nThe relationship between the outcome and the predictor is governed by a single parameter"
  },
  {
    "objectID": "05_slides.html#the-regression-equation",
    "href": "05_slides.html#the-regression-equation",
    "title": "Linear Regression",
    "section": "The regression equation",
    "text": "The regression equation\n\\[\nh_i = \\alpha + \\beta \\, d_i + \\varepsilon_i\n\\]\n\n\\(h_i\\): the height of individual \\(i\\) (one of 1,517 observations)\n\\(d_i\\): the date of birth, expressed in decimal years\n\\(\\alpha\\): the intercept — the predicted average height when \\(d_i = 0\\)\n\\(\\beta\\): the slope — the predicted change in average height for a one-year increase in DOB\n\\(\\varepsilon_i\\): the error term — the deviation of a person’s height from the average height of someone born on their birthday"
  },
  {
    "objectID": "05_slides.html#ordinary-least-squares",
    "href": "05_slides.html#ordinary-least-squares",
    "title": "Linear Regression",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\nWe estimate \\(\\alpha\\) and \\(\\beta\\) by minimizing the sum of squared errors:\n\\[\n\\min_{\\alpha,\\,\\beta} \\sum_{i=1}^{1517} \\varepsilon_i^2\n\\]\nwhere\n\\[\n\\varepsilon_i^2 = (h_i - \\alpha - \\beta \\, d_i)^2\n\\]\n\nWe pick the values of \\(\\alpha\\) and \\(\\beta\\) that make the squared deviations as small as possible\nThere are closed form solutions to this model (you cold solve by hand) but your computer can do it trivially."
  },
  {
    "objectID": "05_slides.html#our-first-regression",
    "href": "05_slides.html#our-first-regression",
    "title": "Linear Regression",
    "section": "Our first regression",
    "text": "Our first regression\n\n\nFigure 4: Height versus date of birth with OLS regression line and 95% confidence band."
  },
  {
    "objectID": "05_slides.html#predicting-off-the-support",
    "href": "05_slides.html#predicting-off-the-support",
    "title": "Linear Regression",
    "section": "Predicting off the support",
    "text": "Predicting off the support\n\nThe slope \\(\\hat\\beta\\) is 0.119 cm per year of birth — slightly positive\nWhat if we predict height for someone born on 2026-01-01?\n\n\\[\n\\hat{h}_{2026\\text{-}01\\text{-}01} = \\hat\\alpha + \\hat\\beta \\, d_{2026\\text{-}01\\text{-}01}\n\\]\n\nOur prediction: 191.9 cm — much too tall for average height!\nThe linear relationship holds across the dates we observe, but the machine can predict at any date\nPredictions become less reliable the farther we move from observed dates\nThis is called predicting off the support of the distribution"
  },
  {
    "objectID": "05_slides.html#errors-vs-residuals",
    "href": "05_slides.html#errors-vs-residuals",
    "title": "Linear Regression",
    "section": "Errors vs residuals",
    "text": "Errors vs residuals\nAn important distinction:\n\nErrors \\(\\varepsilon_i = h_i - \\alpha - \\beta \\, d_i\\): the difference between a person’s height and the true conditional average. We never observe these because we never know the true \\(\\alpha\\) and \\(\\beta\\).\nResiduals \\(\\hat\\varepsilon_i = h_i - \\hat\\alpha - \\hat\\beta \\, d_i\\): the difference between observed heights and the estimated regression line. We do observe these.\nThe residuals are our best available stand-in for the unknown errors\nOur measures of uncertainty are themselves estimates, built from residuals rather than true errors"
  },
  {
    "objectID": "05_slides.html#statistical-uncertainty",
    "href": "05_slides.html#statistical-uncertainty",
    "title": "Linear Regression",
    "section": "Statistical uncertainty",
    "text": "Statistical uncertainty\n\nThe regression coefficients come from a sample and are therefore uncertain\nJust as with a mean, we need a standard error\nRecall: for the sample mean \\(\\bar{h}\\), the standard error is\n\n\\[\n\\text{SE}(\\bar{h}) = \\frac{\\hat\\sigma}{\\sqrt{n}}\n\\]\n\nMore noise \\(\\rightarrow\\) less certain; more data \\(\\rightarrow\\) more certain"
  },
  {
    "objectID": "05_slides.html#standard-error-of-a-slope",
    "href": "05_slides.html#standard-error-of-a-slope",
    "title": "Linear Regression",
    "section": "Standard error of a slope",
    "text": "Standard error of a slope\nThe standard error for a regression slope follows the same logic:\n\\[\n\\text{SE}(\\hat\\beta) = \\sqrt{\\frac{\\hat\\sigma^2}{\\sum_{i=1}^{n}(d_i - \\bar{d})^2}}\n\\]\nwhere \\(\\hat\\sigma^2 = \\frac{1}{n-2}\\sum_{i=1}^{n}\\hat\\varepsilon_i^2\\)\n\nNumerator: how noisy the data are around the regression line\nDenominator: how spread out the predictor is along the x-axis"
  },
  {
    "objectID": "05_slides.html#why-the-spread-of-the-predictor-matters",
    "href": "05_slides.html#why-the-spread-of-the-predictor-matters",
    "title": "Linear Regression",
    "section": "Why the spread of the predictor matters",
    "text": "Why the spread of the predictor matters\n\nA regression slope measures a rate of change (cm per year of DOB)\nTo pin down a rate of change, what matters is not just how many people we observe but how spread out they are along the x-axis\nTwo datasets, both with 1,000 observations:\n\nFirst: everyone born within a single month\nSecond: births spread over a decade\n\nThe second dataset is far more informative about the slope\n\\(\\sum(d_i - \\bar{d})^2\\) captures this: larger when dates are more spread out, making \\(\\text{SE}(\\hat\\beta)\\) smaller"
  },
  {
    "objectID": "05_slides.html#confidence-intervals",
    "href": "05_slides.html#confidence-intervals",
    "title": "Linear Regression",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\\[\n\\hat\\beta \\pm 2 \\times \\text{SE}(\\hat\\beta)\n\\]\n\nIf we repeated our study many times, approximately 95% of these intervals would contain the true value of \\(\\beta\\)\nAny single interval either contains the truth or it doesn’t, but the procedure is right 95% of the time\nWhen zero falls inside the interval: data are consistent with no relationship\nWhen zero falls outside the interval: data suggest the true slope is different from zero"
  },
  {
    "objectID": "05_slides.html#the-t-statistic",
    "href": "05_slides.html#the-t-statistic",
    "title": "Linear Regression",
    "section": "The t-statistic",
    "text": "The t-statistic\n\\[\nt = \\frac{\\hat\\beta}{\\text{SE}(\\hat\\beta)}\n\\]\n\nHow many standard errors our estimate is away from zero\nRule of thumb: \\(|t| &gt; 2\\) means “statistically significant” at conventional levels\nEquivalent to saying zero lies outside the 95% confidence interval\nA coefficient of 0.5 with SE of 0.1 (\\(t = 5\\)) is much more convincing than a coefficient of 0.5 with SE of 0.4 (\\(t = 1.25\\))"
  },
  {
    "objectID": "05_slides.html#reading-a-regression-table",
    "href": "05_slides.html#reading-a-regression-table",
    "title": "Linear Regression",
    "section": "Reading a regression table",
    "text": "Reading a regression table\n\n\n\n\nTable 2: Simple regression of height on date of birth.\n\n\n\n\n\n\n\n\nHeight (cm)\n\n\n\n\nIntercept\n−49.120\n\n\n\n(118.754)\n\n\nDate of birth (year)\n0.119*\n\n\n\n(0.065)\n\n\nNum.Obs.\n1517\n\n\nR2\n0.002\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "05_slides.html#elements-of-a-regression-table",
    "href": "05_slides.html#elements-of-a-regression-table",
    "title": "Linear Regression",
    "section": "Elements of a regression table",
    "text": "Elements of a regression table\n\nCoefficient estimates: each row is a variable; the number is the point estimate\nStandard errors in parentheses: below each coefficient, indicating precision\nStars: * means \\(p &lt; 0.1\\), ** means \\(p &lt; 0.05\\), *** means \\(p &lt; 0.01\\)\nIntercept: predicted height when DOB = 0 (not meaningful here, but necessary)\n\\(R^2\\): how much variation in height is explained by DOB alone (close to 0 = very little)\nNum.Obs. (\\(N\\)): the number of observations"
  },
  {
    "objectID": "05_slides.html#adding-another-regressor",
    "href": "05_slides.html#adding-another-regressor",
    "title": "Linear Regression",
    "section": "Adding another regressor",
    "text": "Adding another regressor\nWe can include both DOB and gender in a multiple regression:\n\\[\n\\text{height}_i = \\alpha + \\beta_1 \\cdot \\text{dob}_i + \\beta_2 \\cdot \\mathbf{1}[\\text{female}_i] + \\varepsilon_i\n\\]\n\n\\(\\mathbf{1}[\\text{female}_i]\\) is an indicator variable (equals 1 if female, 0 if male)\nFor a male: predicted height is \\(\\alpha + \\beta_1 d\\)\nFor a female: predicted height is \\(\\alpha + \\beta_1 d + \\beta_2\\)\n\\(\\beta_2\\) measures the average height difference for females relative to males"
  },
  {
    "objectID": "05_slides.html#holding-constant",
    "href": "05_slides.html#holding-constant",
    "title": "Linear Regression",
    "section": "“Holding constant”",
    "text": "“Holding constant”\nMultiple regression estimates each coefficient holding the other variables constant:\n\n\\(\\beta_1\\): the effect of DOB on height holding gender constant — comparing people of the same gender born at different dates\n\\(\\beta_2\\): the average height difference for females relative to males holding DOB constant — comparing men and women born at the same time"
  },
  {
    "objectID": "05_slides.html#visualizing-holding-constant",
    "href": "05_slides.html#visualizing-holding-constant",
    "title": "Linear Regression",
    "section": "Visualizing “holding constant”",
    "text": "Visualizing “holding constant”\n\n\nFigure 5: How multiple regression isolates the DOB effect after removing gender. Panel A: raw data coloured by gender, with group means marked. Panel B: after subtracting each group’s mean height, the two clouds overlap vertically; group mean DOB is marked. Panel C: after also subtracting each group’s mean DOB, the data are centred at the origin. Panel D: the regression slope through the doubly-residualized data equals \\(\\hat\\beta_1\\) from the multiple regression."
  },
  {
    "objectID": "05_slides.html#what-the-panels-show",
    "href": "05_slides.html#what-the-panels-show",
    "title": "Linear Regression",
    "section": "What the panels show",
    "text": "What the panels show\n\nPanel A: Raw data coloured by gender, with group mean heights marked\nPanel B: After subtracting each group’s mean height, the two clouds overlap vertically\nPanel C: After also subtracting each group’s mean DOB, both variables are “cleaned” of gender\nPanel D: The regression slope through this doubly-residualized data equals \\(\\hat\\beta_1\\) from the multiple regression (Frisch-Waugh-Lovell theorem)\nMultiple regression is, at its core, about comparing like with like"
  },
  {
    "objectID": "05_slides.html#multiple-regression-table",
    "href": "05_slides.html#multiple-regression-table",
    "title": "Linear Regression",
    "section": "Multiple regression table",
    "text": "Multiple regression table\n\n\n\n\nTable 3: Regression results: simple and multiple regression.\n\n\n\n\n\n\n\n\n (1)\n  (2)\n\n\n\n\nIntercept\n−49.120\n−141.882\n\n\n\n(118.754)\n(100.340)\n\n\nDate of birth (year)\n0.119*\n0.171***\n\n\n\n(0.065)\n(0.055)\n\n\nFemale\n\n−10.739***\n\n\n\n\n(0.434)\n\n\nNum.Obs.\n1517\n1517\n\n\nR2\n0.002\n0.289\n\n\nR2 Adj.\n0.002\n0.288\n\n\n\n * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "05_slides.html#comparing-the-two-columns",
    "href": "05_slides.html#comparing-the-two-columns",
    "title": "Linear Regression",
    "section": "Comparing the two columns",
    "text": "Comparing the two columns\n\nDOB coefficient increases from (1) to (2): the simple regression was attenuated by mixing men and women. The standard error falls because including gender shrinks the residuals.\nFemale coefficient is large and negative: the well-known average height difference between men and women. Statistically significant at the 1% level.\n\\(R^2\\) jumps substantially: DOB alone explains little, but adding gender explains the ~10 cm gap between men and women.\nAdjusted \\(R^2\\) penalizes for number of predictors. When it rises meaningfully, the added variable genuinely improves fit."
  },
  {
    "objectID": "05_slides.html#key-takeaways",
    "href": "05_slides.html#key-takeaways",
    "title": "Linear Regression",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nRegression is a way of comparing averages — it computes a linear approximation to the conditional mean\nOLS picks the line that minimizes the sum of squared errors\nStandard errors tell us about the precision of our estimates\nMultiple regression estimates each coefficient holding other variables constant\nAlways check: is the relationship linear? Are you predicting off the support?"
  },
  {
    "objectID": "05_slides.html#bibliography",
    "href": "05_slides.html#bibliography",
    "title": "Linear Regression",
    "section": "Bibliography",
    "text": "Bibliography\n\n\n\n\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781139161879."
  },
  {
    "objectID": "05_slides.html#todays-plan",
    "href": "05_slides.html#todays-plan",
    "title": "Linear Regression",
    "section": "Today’s plan",
    "text": "Today’s plan\n\nAn overview of linear regression\nClass work on the high-wage economy debate"
  },
  {
    "objectID": "05_slides.html#our-goal-conditional-averages",
    "href": "05_slides.html#our-goal-conditional-averages",
    "title": "Linear Regression",
    "section": "Our goal: conditional averages",
    "text": "Our goal: conditional averages\n\nThe best case: so many observations per date that we could calculate the average for each day\nThis would be the conditional average: the average conditional on the day a person was born\nWe don’t have enough data for this, but at least we know our goal is a conditional average\nSolution: calculate an average that depends on the date and a very small number of unknown parameters"
  }
]