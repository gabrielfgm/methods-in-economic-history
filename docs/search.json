[
  {
    "objectID": "03_slides.html#the-plan",
    "href": "03_slides.html#the-plan",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The plan",
    "text": "The plan\n\nLocation, or central tendency\nScale, or spread\nFrom populations to samples\nSampling distributions and standard errors"
  },
  {
    "objectID": "03_slides.html#how-do-we-summarize-a-group",
    "href": "03_slides.html#how-do-we-summarize-a-group",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "How do we summarize a group?",
    "text": "How do we summarize a group?\n\n“Ladders were made to match the height of the enemy’s wall, which they measured by the layers of bricks, the side turned towards them not being thoroughly whitewashed. These were counted by many persons at once; and though some might miss the right calculation, most would hit upon it, particularly as they counted over and over again, and were no great way from the wall, but could see it easily enough for their purpose. The length required for the ladders was thus obtained, being calculated from the breadth of the brick.”\n– Thucydides, in Stigler (2016), p. 30-31"
  },
  {
    "objectID": "03_slides.html#averaging-not-a-universally-known-solution",
    "href": "03_slides.html#averaging-not-a-universally-known-solution",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Averaging not a universally known solution",
    "text": "Averaging not a universally known solution\nPer Stigler (2016):\n\nClear evidence of the use of mean in summarizing scientific measurements in late 1660s\nHipparchus (~150 BCE) or Ptolemy (150 CE) don’t seem to know\nal-Biruni (~1000 CE) reports midpoint between min and max"
  },
  {
    "objectID": "03_slides.html#an-early-example",
    "href": "03_slides.html#an-early-example",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "An early example",
    "text": "An early example\n\nKöbel’s depiction of the determination of the lawful rod (Köbel 1522) cited in Stigler (2016), p. 32"
  },
  {
    "objectID": "03_slides.html#finding-the-center",
    "href": "03_slides.html#finding-the-center",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Finding the “center”",
    "text": "Finding the “center”\nThe problem: we want a single number that best represents a whole collection of data.\nThree common answers:\n\nMode – the most frequent value\nMedian – the middle value\nMean – the arithmetic average\n\nThese can give very different answers!"
  },
  {
    "objectID": "03_slides.html#the-mode",
    "href": "03_slides.html#the-mode",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The mode",
    "text": "The mode\nDefinition: The value that appears most frequently in the dataset.\n\nIn Thucydides bricks example, it is the most commonly counted number of bricks\nCan be useful if your object is to be exactly right\nCons: Unstable. Some datasets have no mode or multiple modes\n\nImagine I measure everyone’s height to the nearest micron (millionth of a metre)."
  },
  {
    "objectID": "03_slides.html#the-median",
    "href": "03_slides.html#the-median",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The median",
    "text": "The median\nDefinition: The value that splits the dataset exactly in half. 50% of the data is above it, 50% is below it.\n\nVisual: Line everyone up by height; the person in the exact middle is the median.\n\nIf there are an even number of people we typically average the ‘two in the middle’\n\nPros: “Robust” – it ignores extreme outliers. If the richest person in the world walks into a room, the median income at most moves by one person.\nCons: Harder to use in mathematical formulas. Sometimes we want to put more weight on extreme values."
  },
  {
    "objectID": "03_slides.html#the-mean-building-notation",
    "href": "03_slides.html#the-mean-building-notation",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The mean: building notation",
    "text": "The mean: building notation\nDefinition: The arithmetic average.\n\nImagine a list of numbers: wages, ages, prices. Call this list \\(x\\), and we refer to the \\(i\\)th item of this list as \\(x_i\\)\nThe first number is \\(x_1\\), the second is \\(x_2\\), and so on.\nWe need to add them all up. Instead of writing “Sum,” we use the Greek letter Sigma: \\(\\Sigma\\)\nIf we have \\(n\\) items, the mean (written \\(\\bar{x}\\), pronounced “x-bar”) is:\n\n\\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\]\nTranslation: “Add everything up and divide by the count.”"
  },
  {
    "objectID": "03_slides.html#why-the-mean-1.-the-mean-as-a-balance-point",
    "href": "03_slides.html#why-the-mean-1.-the-mean-as-a-balance-point",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Why the mean? 1. The mean as a balance point",
    "text": "Why the mean? 1. The mean as a balance point\n\nPhysical analogy: If you placed weights on a seesaw at the location of each data point, the mean is the exact spot where the fulcrum must be placed for the seesaw to balance perfectly.\nUnlike median and mode, the mean is sensitive to extreme values (outliers)\nEvery observation “pulls” on the mean"
  },
  {
    "objectID": "03_slides.html#why-the-mean-2.-sometimes-you-want-the-outliers",
    "href": "03_slides.html#why-the-mean-2.-sometimes-you-want-the-outliers",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Why the mean? 2. Sometimes you want the outliers",
    "text": "Why the mean? 2. Sometimes you want the outliers\nImagine a game where I roll a fair die and if it is any number other than 6 I pay you £5. If I roll a six you pay me £100.\n\nModal earnings from this game: £5\nMedian earnings from this game: £5\nAverage earnings from this game: -£12.50!\nThe rare outcome matters a lot!"
  },
  {
    "objectID": "03_slides.html#when-do-these-differ",
    "href": "03_slides.html#when-do-these-differ",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "When do these differ?",
    "text": "When do these differ?\n\nMean, median, and mode can differ substantially in skewed distributions"
  },
  {
    "objectID": "03_slides.html#when-are-they-the-same",
    "href": "03_slides.html#when-are-they-the-same",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "When are they the same?",
    "text": "When are they the same?\n\nWhen the distribution is symmetric and unimodal\n\n\nTwo symmetric and unimodal distributions: Normal and t-distribution"
  },
  {
    "objectID": "03_slides.html#comparing-the-measures",
    "href": "03_slides.html#comparing-the-measures",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Comparing the measures",
    "text": "Comparing the measures\n\n\n\n\n\n\n\n\n\nFeature\nMode\nMedian\nMean\n\n\n\n\nIntuition\nMost typical\nThe middle\nBalance point\n\n\nOutliers\nIgnores them\nIgnores them (Robust)\nHeavily affected\n\n\nMath use\nDifficult\nHarder to manipulate\nEasiest in formulas"
  },
  {
    "objectID": "03_slides.html#why-is-the-mean-so-dominant",
    "href": "03_slides.html#why-is-the-mean-so-dominant",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Why is the mean so dominant?",
    "text": "Why is the mean so dominant?\n\nLots of contexts (e.g. finance) where the mean is good because it penalizes big errors.\nEase of calculation: Before computers, calculating the median required sorting all the data (very slow for big lists). Calculating the mean just required keeping a running total.\n\nAlso the function minimizing the squared errors is much easier to work with than the function minimizing the absolute errors."
  },
  {
    "objectID": "03_slides.html#the-location-and-its-errors",
    "href": "03_slides.html#the-location-and-its-errors",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The location and its “errors”",
    "text": "The location and its “errors”\nImagine you must pick one number to predict the next value in a dataset. You will be fined based on how wrong you are.\nWe call your miss the ‘error’ \\(e_i = x_i - g\\) where \\(x_i\\) is the actual value and \\(g\\) is your guess.\nThree different penalty rules lead to three different “best” answers:\n\nAll-or-nothing penalty \\(\\rightarrow\\) Mode\nLinear penalty \\(\\rightarrow\\) Median\nSquared penalty \\(\\rightarrow\\) Mean"
  },
  {
    "objectID": "03_slides.html#the-all-or-nothing-game",
    "href": "03_slides.html#the-all-or-nothing-game",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The all-or-nothing game",
    "text": "The all-or-nothing game\nThe rule: If you are exactly right, you pay nothing. If you are wrong by any amount, you pay £100.\nBest strategy: Pick the Mode. (small caveat: the mode needs to exist)\n\nIt gives you the highest probability of hitting the number exactly.\n\nLoss function: \\(L(e) = 100 \\times \\mathbb{I}(e \\neq 0)\\)"
  },
  {
    "objectID": "03_slides.html#the-linear-penalty-game",
    "href": "03_slides.html#the-linear-penalty-game",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The linear penalty game",
    "text": "The linear penalty game\nThe rule: You pay £1 for every unit you are away from the truth. Being off by 10 costs £10; being off by 100 costs £100.\nBest strategy: Pick the Median.\n\nIt minimizes the sum of absolute deviations\nBalances the distance on the left and right sides\nLoss function: \\(L(e) = 100 \\times |e|\\)"
  },
  {
    "objectID": "03_slides.html#the-squared-penalty-game",
    "href": "03_slides.html#the-squared-penalty-game",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The squared penalty game",
    "text": "The squared penalty game\n\nThe rule: You pay based on the square of your error.\n\nOff by 1? Pay £1.\nOff by 10? Pay £100.\nOff by 50? Pay £2,500.\n\nThe logic: Small mistakes are fine, but big mistakes are disastrous.\nBest strategy: Pick the Mean.\n\nIt minimizes the sum of squared errors (Least Squares)\n\nLoss function: \\(L(e) = 100 \\times e^2\\)"
  },
  {
    "objectID": "03_slides.html#from-location-to-spread",
    "href": "03_slides.html#from-location-to-spread",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "From location to spread",
    "text": "From location to spread\n\nThe problem: Knowing the average isn’t enough."
  },
  {
    "objectID": "03_slides.html#the-average-squared-error",
    "href": "03_slides.html#the-average-squared-error",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The average squared error",
    "text": "The average squared error\n\nRecall: The mean is the number that minimizes the sum of squared errors.\nThe question: If the mean is our “best guess,” how good is that guess on average?\n\nCalculate the squared distance of every point from the mean\nAdd them up and divide by the count\nThis gives us a measure of the average dispersion of the data"
  },
  {
    "objectID": "03_slides.html#variance-definition",
    "href": "03_slides.html#variance-definition",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Variance: definition",
    "text": "Variance: definition\n\nDefinition: The Variance (\\(s^2\\)) is the average squared distance from the center.\n\\[s^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\]\nTranslation: “How far, on average, are the data points from the mean (squared)?”\nRemember we defined: \\(e_i = x_i - \\bar{x}\\).\nSo we can rewrite the variance as:\n\\[s^2 = \\frac{1}{n}\\sum_{i=1}^{n} e_i^2\\]"
  },
  {
    "objectID": "03_slides.html#the-units-problem",
    "href": "03_slides.html#the-units-problem",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The units problem",
    "text": "The units problem\n\n\nIf we measure the age at death of Roman Emperors in years, the variance is measured in years squared\nWhat is a “square year”? It doesn’t make intuitive sense historically.\nThis is awkward for interpretating variance on the scale of the data"
  },
  {
    "objectID": "03_slides.html#standard-deviation-returning-to-regular-units",
    "href": "03_slides.html#standard-deviation-returning-to-regular-units",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Standard deviation: returning to regular units",
    "text": "Standard deviation: returning to regular units\n\nThe fix: Take the square root of the variance to put the spread in units that are the same as the data.\nDefinition: The Standard Deviation (\\(s\\))\n\\[s = \\sqrt{s^2} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} e_i^2}\\]\nwhich equals:\n\\[s = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} x_i^2 - \\bar{x}^2}\\]\nInterpretation: Roughly the “average” distance of a data point from the mean."
  },
  {
    "objectID": "03_slides.html#what-does-the-standard-deviation-tell-us",
    "href": "03_slides.html#what-does-the-standard-deviation-tell-us",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "What does the standard deviation tell us?",
    "text": "What does the standard deviation tell us?\n\n\nLow SD: Data is close to the mean\n\nA more homogeneous population\n\nHigh SD: Data is far from the mean\n\nA more heterogeneous population\n\nTwo datasets with the same mean but different SDs describe very different worlds"
  },
  {
    "objectID": "03_slides.html#visualizing-dispersion",
    "href": "03_slides.html#visualizing-dispersion",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Visualizing dispersion",
    "text": "Visualizing dispersion\n\n\n\n\n\n\nTwo distributions with the same mean but different standard deviations"
  },
  {
    "objectID": "03_slides.html#other-measures-of-dispersion",
    "href": "03_slides.html#other-measures-of-dispersion",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Other measures of dispersion",
    "text": "Other measures of dispersion\n\nSometimes (not often) you will see other measures of dispersion e.g.\n\nThe range: the difference between the maximum and minimum value\nThe interquartile range: the difference between the 75th and 25th percentile\nThe mean absolute deviation: the average absolute distance from the mean\nThe median absolute deviation: the median of the absolute distances from the median"
  },
  {
    "objectID": "03_slides.html#means-and-sample-averages",
    "href": "03_slides.html#means-and-sample-averages",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Means and sample averages",
    "text": "Means and sample averages\n\n\nImagine a well defined population like “the heights (in cm) of all students currently registered in the history department at KCL”\n\nImagine this is 1000 students\n\nThis population has a mean which we will call \\(\\mu\\) (it is convention to use the greek letter mu for the population mean)\nThis population has a standard deviation which we will call \\(\\sigma\\) (it is convention to use the greek letter sigma for the population standard deviation)\nI take the heights of the ~20 history students in this class and calculate the average height \\(\\bar{x}\\)\n\nWhat is the relationship between \\(\\bar{x}\\) and \\(\\mu\\)?"
  },
  {
    "objectID": "03_slides.html#means-and-sample-averages-visualized",
    "href": "03_slides.html#means-and-sample-averages-visualized",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Means and sample averages: visualized",
    "text": "Means and sample averages: visualized\n\n\n\n\n\n\nSample vs population"
  },
  {
    "objectID": "03_slides.html#sample-vs-population",
    "href": "03_slides.html#sample-vs-population",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Sample vs population",
    "text": "Sample vs population\n\n\nIn practice we rarely observe the full population\nWe observe a sample and want to learn about the population\nSample statistics are estimates of population parameters\n\n\n\n\nPopulation parameter\nSample statistic\n\n\n\n\n\\(\\mu\\) (mean)\n\\(\\bar{x}\\)\n\n\n\\(\\sigma^2\\) (variance)\n\\(s^2\\)\n\n\n\\(\\sigma\\) (std dev)\n\\(s\\)"
  },
  {
    "objectID": "03_slides.html#the-sampling-distribution",
    "href": "03_slides.html#the-sampling-distribution",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The sampling distribution",
    "text": "The sampling distribution\n\n\nA sample mean is itself subject to sampling error\nIf we drew many samples from the same population, each would give a different \\(\\bar{x}\\)\nThe distribution of these sample means is called the sampling distribution\nThis distribution has its own spread, which we can quantify"
  },
  {
    "objectID": "03_slides.html#the-standard-error-of-the-mean",
    "href": "03_slides.html#the-standard-error-of-the-mean",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The standard error of the mean",
    "text": "The standard error of the mean\n\nThe standard error of the mean (SEM) describes how much sample means vary:\n\\[SE(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}\\]\n\nKey insight: the standard error decreases as sample size increases\nLarger samples give more precise estimates"
  },
  {
    "objectID": "03_slides.html#standard-deviation-vs-standard-error",
    "href": "03_slides.html#standard-deviation-vs-standard-error",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Standard deviation vs standard error",
    "text": "Standard deviation vs standard error\n\n\n\n\n\n\n\n\n\n\nStandard Deviation (SD)\nStandard Error (SE)\n\n\n\n\nMeasures\nSpread of individual observations\nSpread of sample means\n\n\nQuestion\nHow variable is the data?\nHow uncertain is our estimate of the mean?\n\n\nFormula\n\\(s = \\sqrt{\\frac{1}{n}\\sum(x_i - \\bar{x})^2}\\)\n\\(SE = \\frac{s}{\\sqrt{n}}\\)\n\n\nAs n increases\nStays roughly the same\nGets smaller\n\n\nDescribes\nThe population/sample itself\nOur knowledge about the population"
  },
  {
    "objectID": "03_slides.html#intuition-for-the-standard-error",
    "href": "03_slides.html#intuition-for-the-standard-error",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Intuition for the standard error",
    "text": "Intuition for the standard error\n\n\n\n\n\n\nSampling distribution of the mean narrows as sample size increases"
  },
  {
    "objectID": "03_slides.html#why-the-standard-error-matters",
    "href": "03_slides.html#why-the-standard-error-matters",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Why the standard error matters",
    "text": "Why the standard error matters\n\n\nThe standard error lets us quantify uncertainty about our estimate\nLarger samples give more precise estimates (smaller SE)\nThis is the foundation for:\n\nConfidence intervals\nHypothesis testing\nAssessing statistical significance"
  },
  {
    "objectID": "03_slides.html#the-standard-error-in-historical-practice",
    "href": "03_slides.html#the-standard-error-in-historical-practice",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The standard error in historical practice",
    "text": "The standard error in historical practice\n\nConsider a historian estimating average wages in 18th century London:\n\nSample of 500 wage records: \\(\\bar{x}\\) = 12 shillings, \\(s\\) = 4 shillings\n\\(SE = \\frac{4}{\\sqrt{500}} \\approx 0.18\\) shillings\nWe can be confident the true mean is close to 12 shillings\nBut what if we only had 25 records?\n\n\\(SE = \\frac{4}{\\sqrt{25}} = 0.8\\) shillings – much more uncertainty!"
  },
  {
    "objectID": "03_slides.html#statistical-vs-substantive-significance",
    "href": "03_slides.html#statistical-vs-substantive-significance",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Statistical vs substantive significance",
    "text": "Statistical vs substantive significance\n\n\nThe standard error is what is used to determine if something is ‘statistically significantly different’ from another thing (usually zero)\nThat is because confidence intervals are constructed using the standard error (we discussed CIs last week)\nThe bigger your sample the smaller the CI\nMcCloskey and Ziliak (1996) insists we should not confuse statistical significance with substantive significance\n\nStatistical significance is about how precisely we measure something, not how big something is."
  },
  {
    "objectID": "03_slides.html#discussion-questions",
    "href": "03_slides.html#discussion-questions",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Discussion questions",
    "text": "Discussion questions\n\n\nWhen might the median be preferable to the mean in historical research?\nFeinstein and Thomas (2002) and Hudson and Ishizu (2016) present these concepts for historians. Why might historians need different pedagogical approaches than economists?\nHow should we think about the standard error when our data is not a random sample? (Most historical data isn’t!)"
  },
  {
    "objectID": "03_slides.html#exercise-british-wages",
    "href": "03_slides.html#exercise-british-wages",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Exercise: British wages",
    "text": "Exercise: British wages\n\nI will give you data on wages in different historical periods.\n\nCalculate the mean, median, and standard deviation\nCalculate the standard error of the mean\nWhat does the standard error tell you about how confident you should be in your estimate?\nWhat doesn’t the standard error tell you about the quality of your data?"
  },
  {
    "objectID": "03_slides.html#key-takeaways",
    "href": "03_slides.html#key-takeaways",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Key takeaways",
    "text": "Key takeaways\n\n\nLocation (mean, median, mode) tells us where the center is – each minimizes a different type of error\nScale (variance, standard deviation) tells us how spread out the data is\nThe standard error tells us how uncertain we are about our estimate of the mean (it is the standard deviation of our parameter of interest)"
  },
  {
    "objectID": "03_slides.html#bibliography",
    "href": "03_slides.html#bibliography",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Bibliography",
    "text": "Bibliography\n\n\n\n\n\n\n\n\nMcCloskey, Deirdre N., and Stephen T. Ziliak. 1996. “The Standard Error of Regressions.” Journal of Economic Literature 34 (1): 97–114. http://www.jstor.org/stable/2729411.\n\n\nStigler, Stephen M. 2016. The seven pillars of statistical wisdom. Cambridge, Massachusetts London, England: Harvard University Press."
  },
  {
    "objectID": "03_slides.html#means-and-sample-averages-1",
    "href": "03_slides.html#means-and-sample-averages-1",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "Means and sample averages",
    "text": "Means and sample averages\n\n\n\n\n\n\nSample vs population"
  },
  {
    "objectID": "03_slides.html#the-standard-error-and-sample-size",
    "href": "03_slides.html#the-standard-error-and-sample-size",
    "title": "Basic tools: central tendencies and dispersion",
    "section": "The standard error and sample size",
    "text": "The standard error and sample size\n\n\n\n\n\n\nSample mean and ±2 SD for different sample sizes"
  },
  {
    "objectID": "04_slides.html#the-plan",
    "href": "04_slides.html#the-plan",
    "title": "Selection bias",
    "section": "The plan",
    "text": "The plan\n\nWhat is sample selection bias?\nSelection on observables\nSelection on unobservables\nHow selection bias distorts relationships between variables"
  },
  {
    "objectID": "04_slides.html#from-statistical-significance-to-bias",
    "href": "04_slides.html#from-statistical-significance-to-bias",
    "title": "Selection bias",
    "section": "From statistical significance to bias",
    "text": "From statistical significance to bias\n\nLast week we covered means, standard errors, and statistical significance\nThose tools assume our sample is a random draw from the population\nBut what happens when it isn’t?\nIf our sample is not random, our estimates may be biased – systematically wrong in a particular direction\nNo amount of data fixes a biased sample: a bigger biased sample is still biased"
  },
  {
    "objectID": "04_slides.html#the-bigger-threat",
    "href": "04_slides.html#the-bigger-threat",
    "title": "Selection bias",
    "section": "The bigger threat",
    "text": "The bigger threat\n\nIn observational studies the bigger danger is not sampling error but that your sample is systematically unrepresentative of the population\nStatistical significance tells us about precision (how much noise is in our estimate)\nBut bias is about accuracy (whether we are pointing at the right answer)\nYou can have a very precisely measured estimates of the wrong thing"
  },
  {
    "objectID": "04_slides.html#what-is-sample-selection-bias",
    "href": "04_slides.html#what-is-sample-selection-bias",
    "title": "Selection bias",
    "section": "What is sample selection bias?",
    "text": "What is sample selection bias?\nDefinition: Sample selection bias occurs when your sample is not a random draw from the population you want to study. You are more likely to capture some kinds of people rather than others.\n\nThe process that generates your data is not independent of the thing you are trying to measure\nThis means your sample statistics (means, correlations, etc.) may not reflect the true population values"
  },
  {
    "objectID": "04_slides.html#where-does-historical-data-come-from",
    "href": "04_slides.html#where-does-historical-data-come-from",
    "title": "Selection bias",
    "section": "Where does historical data come from?",
    "text": "Where does historical data come from?\n\n\nTax records – only people above a wealth threshold\nMilitary records – only people who enlisted (and met requirements)\nParish registers – only people in that parish, and that religion\nCourt records – only people involved in disputes\nCensus records – depends on who the enumerator found at home\nNewspaper reports – depends on what editors found newsworthy\n\nNone of these are random samples of the population. Each source has its own selection process.\nAlways ask: who is in the sample and why?"
  },
  {
    "objectID": "04_slides.html#a-simple-example-height-and-the-military",
    "href": "04_slides.html#a-simple-example-height-and-the-military",
    "title": "Selection bias",
    "section": "A simple example: height and the military",
    "text": "A simple example: height and the military\n\n\nSuppose we want to estimate average height in the population using military records\nBut the military has a minimum height requirement – say, 165 cm\nEveryone below the cutoff is excluded from our data\nOur sample mean will overestimate the true population mean\nThis is truncation bias: we only observe part of the distribution"
  },
  {
    "objectID": "04_slides.html#truncation-bias-illustrated",
    "href": "04_slides.html#truncation-bias-illustrated",
    "title": "Selection bias",
    "section": "Truncation bias illustrated",
    "text": "Truncation bias illustrated\n\n\n\n\n\n\nMinimum height requirement biases the sample mean upward"
  },
  {
    "objectID": "04_slides.html#aside-truncation-and-worst-case-bounds",
    "href": "04_slides.html#aside-truncation-and-worst-case-bounds",
    "title": "Selection bias",
    "section": "Aside: truncation and worst-case bounds",
    "text": "Aside: truncation and worst-case bounds\n\n\nWhen your sample is truncated (e.g., only observe heights above a threshold), you can place bounds on what the population mean might be\nThe worst case: all the people you don’t observe could be very short (lower bound) or very tall (upper bound)\nTruncation alone is tractable – we can model it if we know the cutoff\nBut the real problem in historical data is usually more subtle: selection depends on things we cannot fully observe"
  },
  {
    "objectID": "04_slides.html#selection-on-observables",
    "href": "04_slides.html#selection-on-observables",
    "title": "Selection bias",
    "section": "Selection on observables",
    "text": "Selection on observables\nDefinition: Selection bias that depends on variables you can see in your data.\n\nIf you know what is causing the selection, you can potentially correct for it\nThe key requirement: the variable driving selection must be measured in your dataset"
  },
  {
    "objectID": "04_slides.html#a-polling-example",
    "href": "04_slides.html#a-polling-example",
    "title": "Selection bias",
    "section": "A polling example",
    "text": "A polling example\n\nSuppose you’re polling voting intentions\nYour sample over-represents university-educated voters (60% of sample vs. 30% of population)\nUniversity-educated voters favour Party A at 70%; non-university voters favour Party A at 40%\nNaive estimate (raw sample): \\(0.6 \\times 70 + 0.4 \\times 40 = 58\\%\\) for Party A\nReweighted estimate (using population shares): \\(0.3 \\times 70 + 0.7 \\times 40 = 49\\%\\) for Party A\nBecause we observed education level, we could fix the bias"
  },
  {
    "objectID": "04_slides.html#reweighting-in-action",
    "href": "04_slides.html#reweighting-in-action",
    "title": "Selection bias",
    "section": "Reweighting in action",
    "text": "Reweighting in action\n\n\n\n\n\n\nReweighting corrects for over-representation of university-educated voters"
  },
  {
    "objectID": "04_slides.html#selection-on-unobservables",
    "href": "04_slides.html#selection-on-unobservables",
    "title": "Selection bias",
    "section": "Selection on unobservables",
    "text": "Selection on unobservables\nDefinition: Selection bias that depends on variables you cannot see in your data.\n\nYou cannot reweight or control for something you haven’t measured\nThis is the hard problem: the bias is invisible in your dataset\nYou need assumptions or external information to address it"
  },
  {
    "objectID": "04_slides.html#the-anthropometrics-problem",
    "href": "04_slides.html#the-anthropometrics-problem",
    "title": "Selection bias",
    "section": "The anthropometrics problem",
    "text": "The anthropometrics problem\n\n\nHistorians use military height records to infer population health and nutrition\nMilitary recruiters selected soldiers partly based on characteristics we can observe (age, occupation)\nBut also on characteristics we cannot observe: health, motivation, economic desperation\nIf unhealthy or desperate people are more likely to enlist (because they lack other options), the height sample may be biased downward\nAnd you can’t fix this by reweighting – because you don’t observe the variable driving selection"
  },
  {
    "objectID": "04_slides.html#the-industrialization-puzzle",
    "href": "04_slides.html#the-industrialization-puzzle",
    "title": "Selection bias",
    "section": "The industrialization puzzle",
    "text": "The industrialization puzzle\n\n\nBodenhorn, Guinnane, and Mroz (2017) argue that US military height samples are selected on unobservables\nThe observed decline in heights during industrialization may be an artefact of changing selection patterns, not a real decline in nutrition\nAs the economy industrialized, the type of person who enlisted changed\nIf selection into the military became less favourable over time (e.g., more desperate, less healthy recruits), measured heights would fall even if population health was stable\nKomlos and A’Hearn (2019) dispute this interpretation; Bodenhorn, Guinnane, and Mroz (2019) respond"
  },
  {
    "objectID": "04_slides.html#the-heckman-correction-the-problem",
    "href": "04_slides.html#the-heckman-correction-the-problem",
    "title": "Selection bias",
    "section": "The Heckman correction: the problem",
    "text": "The Heckman correction: the problem\n\n\nWe want to estimate average height in the population\nBut we only observe heights of people who were selected into the military\nThe selection process is correlated with height itself (or with things correlated with height)\nSimply averaging observed heights gives a biased answer\nWe need some way to account for the selection process"
  },
  {
    "objectID": "04_slides.html#the-heckman-correction-the-intuition",
    "href": "04_slides.html#the-heckman-correction-the-intuition",
    "title": "Selection bias",
    "section": "The Heckman correction: the intuition",
    "text": "The Heckman correction: the intuition\n\n\nHeckman’s insight: Model the selection process separately from the outcome\nUse information about who gets selected to adjust your estimates of the outcome\nAnalogy: Imagine you only see wages of people who are employed. To estimate what wages would look like for everyone, you first need to model who chooses to work and who doesn’t. The factors that predict employment (but not wages directly) help you correct for the bias.\nLimitation: Requires assumptions about the functional form of the selection process – if those assumptions are wrong, the correction can be wrong too\nBodenhorn, Guinnane, and Mroz (2017) use a Heckman-style correction; Komlos and A’Hearn (2019) dispute whether the assumptions are justified"
  },
  {
    "objectID": "04_slides.html#selection-bias-and-relationships-between-variables",
    "href": "04_slides.html#selection-bias-and-relationships-between-variables",
    "title": "Selection bias",
    "section": "Selection bias and relationships between variables",
    "text": "Selection bias and relationships between variables\n\nSo far we’ve focused on how selection bias shifts averages\nBut selection bias can also distort relationships between variables\nThis is sometimes called Berkson’s paradox or collider bias\nThe key insight: conditioning on a variable that is caused by two other variables can create a spurious correlation between them"
  },
  {
    "objectID": "04_slides.html#berksons-paradox-the-nba-example",
    "href": "04_slides.html#berksons-paradox-the-nba-example",
    "title": "Selection bias",
    "section": "Berkson’s paradox: the NBA example",
    "text": "Berkson’s paradox: the NBA example\n\nAmong NBA players, taller players tend to have worse free-throw percentages\nBut in the general population, height has no relationship to free-throw accuracy\nThe NBA selects players who are either very tall or very accurate shooters (or both)\nAmong the selected group, if you’re not tall, you must be an amazing shooter (otherwise you wouldn’t be in the NBA)\nThis creates a spurious negative correlation in the selected sample that doesn’t exist in the population"
  },
  {
    "objectID": "04_slides.html#berksons-paradox-illustrated",
    "href": "04_slides.html#berksons-paradox-illustrated",
    "title": "Selection bias",
    "section": "Berkson’s paradox illustrated",
    "text": "Berkson’s paradox illustrated\n\nSelection creates a spurious negative correlation (Berkson’s paradox)"
  },
  {
    "objectID": "04_slides.html#connection-to-the-anthropometrics-debate",
    "href": "04_slides.html#connection-to-the-anthropometrics-debate",
    "title": "Selection bias",
    "section": "Connection to the anthropometrics debate",
    "text": "Connection to the anthropometrics debate\n\n\nIn the height debates, if selection patterns changed over time (e.g., the military became less selective during wartime), the relationship between height and time is distorted\nIf during peacetime only the tallest and healthiest enlist, but during wartime the military takes almost everyone, the average height of recruits falls – but not because the population got shorter\nThis is the same logic as Berkson’s paradox: conditioning on selection distorts the relationship between the variables of interest\nThis is what makes selection on unobservables so dangerous for historical inference"
  },
  {
    "objectID": "04_slides.html#key-takeaways",
    "href": "04_slides.html#key-takeaways",
    "title": "Selection bias",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nSample selection bias is often a bigger threat than sampling error in historical research\nSelection on observables can be corrected if you measure the relevant variables\nSelection on unobservables requires strong assumptions to correct (e.g., Heckman’s joint normality)\nSelection can distort relationships between variables (Berkson’s paradox), not just averages\nAlways ask: who is in the sample and why?"
  },
  {
    "objectID": "04_slides.html#bibliography",
    "href": "04_slides.html#bibliography",
    "title": "Selection bias",
    "section": "Bibliography",
    "text": "Bibliography\n\n\n\n\n\n\n\n\nManski, Charles F. 2007. Identification for Predication and Decision. Harvard University Press."
  },
  {
    "objectID": "04_slides.html#a-simple-example-height-and-the-royal-marines",
    "href": "04_slides.html#a-simple-example-height-and-the-royal-marines",
    "title": "Selection bias",
    "section": "A simple example: height and the Royal Marines",
    "text": "A simple example: height and the Royal Marines\n\nSuppose we want to estimate average height in the population using military records\nThe Royal Marines require a minimum height of 145 cm\nEveryone below the cutoff is excluded from our data\nOur sample mean will overestimate the true population mean\nThis is truncation bias: we only observe part of the distribution"
  },
  {
    "objectID": "04_slides.html#worst-case-bounds-and-missing-data",
    "href": "04_slides.html#worst-case-bounds-and-missing-data",
    "title": "Selection bias",
    "section": "Worst-case bounds and missing data",
    "text": "Worst-case bounds and missing data\nManski (2007) : A survey contacts 137 unhoused people and follows up later to ask whether they found housing.\n\nAt follow-up, only 78 respond. Of these, 24 had exited homelessness.\n59 non-respondents: did they find housing? We don’t know.\nLower bound: Assume all 59 non-respondents did not exit \\(\\rightarrow\\) \\(24/137 = 17.5\\%\\)\nUpper bound: Assume all 59 non-respondents did exit \\(\\rightarrow\\) \\((24 + 59)/137 = 60.6\\%\\)\nNaive estimate (respondents only): \\(24/78 = 30.8\\%\\)\nThe true answer lies somewhere in \\([17.5\\%, 60.6\\%]\\) – a wide range, but an honest one"
  },
  {
    "objectID": "04_slides.html#correcting-for-selection-on-unobservables",
    "href": "04_slides.html#correcting-for-selection-on-unobservables",
    "title": "Selection bias",
    "section": "Correcting for selection on unobservables",
    "text": "Correcting for selection on unobservables\n\nThe Heckman correction attempts to fix selection on unobservables\nKey assumption: the errors in the outcome equation and the selection equation are jointly normally distributed\nHigh level overview: you assume a specific formula describes the relationship between the unobservables driving selection and the outcome."
  },
  {
    "objectID": "04_slides.html#the-heckman-correction-in-action",
    "href": "04_slides.html#the-heckman-correction-in-action",
    "title": "Selection bias",
    "section": "The Heckman correction in action",
    "text": "The Heckman correction in action\n\n\n\n\n\n\nUnder joint normality, the Heckman correction recovers the true mean"
  },
  {
    "objectID": "04_slides.html#worse-case-bounds",
    "href": "04_slides.html#worse-case-bounds",
    "title": "Selection bias",
    "section": "Worse-case bounds",
    "text": "Worse-case bounds"
  }
]